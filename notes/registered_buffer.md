Great question â€” this is an important PyTorch concept, especially for things like masks and positional encodings.

---

### ğŸ§© What a â€œregistered bufferâ€ is

A **registered buffer** is a tensor thatâ€™s **part of a moduleâ€™s state**, but **isnâ€™t a trainable parameter** (i.e. it wonâ€™t show up in `model.parameters()` or get gradients).

You register it with:

```python
self.register_buffer("name", tensor)
```

That tells PyTorch:

* âœ… it **belongs to the module** (so itâ€™s saved/loaded with `state_dict()`);
* âœ… it **moves automatically** when you call `model.to(device)` or `model.cuda()` â€” no need to manually do `tensor.to(device)`;
* ğŸš« but it **does not require gradients** and is **not updated** by the optimizer.

---

### ğŸ’¡ When to use it

Use a registered buffer for **constant tensors** that are part of the model definition but depend on device or dtype, for example:

* causal masks (`tril`)
* positional encodings (fixed)
* normalization statistics (non-learnable)
* padding masks or lookup tables

---

### âš™ï¸ Example

Without buffer (bug-prone):

```python
self.tril = torch.tril(torch.ones(block_size, block_size))
# later in forward
attention = attention.masked_fill(self.tril[:T, :T] == 0, float('-inf'))
# âŒ self.tril stays on CPU; mismatch if model moved to GPU or MPS
```

With buffer (robust):

```python
self.register_buffer("tril", torch.tril(torch.ones(block_size, block_size)))
# automatically on correct device when you call model.to("mps")
attention = attention.masked_fill(self.tril[:T, :T] == 0, float('-inf'))
```

---

### ğŸ” You can inspect buffers like this:

```python
list(model.buffers())
```

and see them appear in:

```python
model.state_dict().keys()
```

but **not** in:

```python
model.parameters()
```

---

In short:

> ğŸ§  A registered buffer is a non-trainable tensor that moves with your model and is saved with its weights â€” perfect for masks and constants.

Would you like me to show how your `SelfAttentionHead` should define and use a registered buffer line-by-line?
